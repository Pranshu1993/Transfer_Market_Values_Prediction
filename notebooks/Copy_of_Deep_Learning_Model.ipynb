{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/Cf1MBMm/0mVL6ZGqlpKu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pranshu1993/Transfer_Market_Values_Prediction/blob/master/notebooks/Copy_of_Deep_Learning_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyper Tuning**"
      ],
      "metadata": {
        "id": "D3oFz01RJVUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from kerastuner.tuners import RandomSearch\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data from a CSV file\n",
        "data = pd.read_csv('/content/Feature_selected.csv')\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_data = data.sample(frac=0.8, random_state=0)\n",
        "test_data = data.drop(train_data.index)\n",
        "\n",
        "# Separate the target variable from the features\n",
        "X_train = train_data.drop(['Market Values'], axis=1)\n",
        "y_train = train_data['Market Values']\n",
        "X_test = test_data.drop(['Market Values'], axis=1)\n",
        "y_test = test_data['Market Values']\n",
        "\n",
        "# Define the model architecture\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    for i in range(hp.Int('num_layers', 2, 20)):\n",
        "        model.add(layers.Dense(units=hp.Int('units_' + str(i), min_value=32, max_value=512, step=32),\n",
        "                               activation=hp.Choice('activation_' + str(i), values=['relu', 'tanh'])))\n",
        "    model.add(layers.Dense(1, activation='linear'))\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
        "        loss='mse',\n",
        "        metrics=['mse'])\n",
        "    return model\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=3,\n",
        "    directory='my_dir',\n",
        "    project_name='helloworld')\n",
        "\n",
        "# Perform the hyperparameter search\n",
        "tuner.search(X_train, y_train,\n",
        "             epochs=50,\n",
        "             validation_data=(X_test, y_test))\n",
        "\n",
        "# Print the best hyperparameters\n",
        "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"Best hyperparameters: {best_hp}\")\n",
        "\n",
        "# Trial 10 Complete [00h 09m 24s]\n",
        "# val_loss: 334365116596224.0\n",
        "\n",
        "# Best val_loss So Far: 332913406465365.3\n",
        "# Total elapsed time: 00h 51m 41s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7CNkQzlJTeR",
        "outputId": "b5139b8a-099b-4e89-b296-c30676e4de62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: <keras_tuner.engine.hyperparameters.hyperparameters.HyperParameters object at 0x7f220cad76d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Reshape, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/Feature_selected.csv\")\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(df, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the dimensions of the noise input vector\n",
        "noise_dim = 100\n",
        "\n",
        "# Build the generator model\n",
        "generator = Sequential()\n",
        "generator.add(Dense(256, input_dim=noise_dim, activation='relu'))\n",
        "generator.add(Dense(512, activation='relu'))\n",
        "generator.add(Dense(1024, activation='relu'))\n",
        "generator.add(Dense(df.shape[1], activation='linear'))\n",
        "generator.add(Reshape((df.shape[1], 1)))\n",
        "\n",
        "# Build the discriminator model\n",
        "discriminator = Sequential()\n",
        "discriminator.add(Flatten(input_shape=(df.shape[1], 1)))\n",
        "discriminator.add(Dense(512, activation='relu'))\n",
        "discriminator.add(Dense(256, activation='relu'))\n",
        "discriminator.add(Dense(1, activation='sigmoid'))\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
        "\n",
        "# Build the GAN model\n",
        "discriminator.trainable = False\n",
        "gan = Sequential()\n",
        "gan.add(generator)\n",
        "gan.add(discriminator)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
        "\n",
        "# Train the GAN model\n",
        "epochs = 100\n",
        "batch_size = train_data.shape[0] # Set batch size equal to number of samples in train_data\n",
        "for epoch in range(epochs):\n",
        "    # Generate random noise as input to the generator\n",
        "    noise = np.random.normal(0, 1, size=(batch_size, noise_dim))\n",
        "\n",
        "    # Generate fake samples using the generator\n",
        "    fake_data = generator.predict(noise)\n",
        "    train_data_3D = train_data.values.reshape(train_data.shape[0], train_data.shape[1], 1)\n",
        "    \n",
        "    fake_data_3D = fake_data.reshape(fake_data.shape[0], fake_data.shape[1], 1)\n",
        "    data_combined = np.concatenate((train_data_3D, fake_data_3D))\n",
        "\n",
        "    # Labels for fake and real samples\n",
        "    labels_combined = np.concatenate((np.zeros((batch_size, 1)), np.ones((batch_size, 1))))\n",
        "\n",
        "    # Train the discriminator\n",
        "    discriminator.trainable = True\n",
        "    discriminator.train_on_batch(data_combined, labels_combined)\n",
        "\n",
        "    # Train the generator\n",
        "    noise = np.random.normal(0, 1, size=(batch_size, noise_dim))\n",
        "    labels_mislabeled = np.ones((batch_size, 1))\n",
        "    discriminator.trainable = False\n",
        "    gan.train_on_batch(noise, labels_mislabeled)\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "# Generate predictions using the generator\n",
        "# Generate predictions using the generator\n",
        "noise = np.random.normal(0, 1, size=(test_data.shape[0], noise_dim))\n",
        "predictions = generator.predict(noise)\n",
        "predictions = predictions.reshape(test_data.shape[0], -1) * (df[\"Market Values\"].max() - df[\"Market Values\"].min()) + df[\"Market Values\"].min()\n",
        "\n",
        "# Calculate mean squared error\n",
        "mse = mean_squared_error(test_data, predictions)\n",
        "print(\"MSE:\", mse)\n",
        "\n",
        "gan.save(\"my_gan_model.h5\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83bfc4d1-80a2-43d3-fe04-7574145a800a",
        "id": "8avDXcyxyjjQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85/85 [==============================] - 1s 7ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 3ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 5ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 5ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 0s 5ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 3ms/step\n",
            "85/85 [==============================] - 0s 5ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 3ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 0s 3ms/step\n",
            "85/85 [==============================] - 0s 5ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 1s 7ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 0s 5ms/step\n",
            "85/85 [==============================] - 0s 6ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "37/37 [==============================] - 0s 4ms/step\n",
            "MSE: 2.7264811127482426e+17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvjxTW3hN6Fl"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Reshape, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "df=pd.read_csv(\"/content/Feature_selected.csv\")\n",
        "# Loading the dataset\n",
        "X =df.drop([\"Market Values\"],axis=1)\n",
        "y =df[\"Market Values\"]   \n",
        " \n",
        "# Splitting\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y,\n",
        "                      test_size = 0.3, random_state = 123)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "v3d5cHazYhom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Reshape, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "df=pd.read_csv(\"/content/Feature_select.csv\")\n",
        "# Loading the dataset\n",
        "X =df.drop([\"Market Value\"],axis=1)\n",
        "y =df[\"Market Value\"]   \n",
        " \n",
        "# Splitting\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y,\n",
        "                      test_size = 0.3, random_state = 123)\n",
        "import numpy as np\n",
        "# Creating an instance of the Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
        "\n",
        "# Training the model\n",
        "rf_regressor.fit(train_X, train_y)\n",
        "\n",
        "# Making predictions on the test set\n",
        "y_pred = rf_regressor.predict(test_X)\n",
        "\n",
        "# Evaluating the model performance\n",
        "mse = np.sqrt(mean_squared_error(test_y, y_pred))\n",
        "print('root Mean Squared Error:', mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtQ587GNYgJt",
        "outputId": "e46fbd62-f73d-4341-9c24-b77a98a7f2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root Mean Squared Error: 11130243.572657559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feed Forward Neural Network**"
      ],
      "metadata": {
        "id": "yPtnBhfBY4yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Reshape, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "df=pd.read_csv(\"/content/Feature_selected.csv\")\n",
        "# Loading the dataset\n",
        "X =df.drop([\"Market Values\"],axis=1)\n",
        "y =df[\"Market Values\"]   \n",
        "# Splitting\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y,\n",
        "                      test_size = 0.3, random_state = 123)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_dim=X.shape[1], activation='LeakyReLU')) # Input layer with 10 neurons\n",
        "model.add(Dense(256, activation='LeakyReLU'))\n",
        "model.add(Dense(128, activation='LeakyReLU')) # Hidden layer with 5 neurons\n",
        "model.add(Dense(64, activation='LeakyReLU'))\n",
        "\n",
        "model.add(Dense(1, activation='linear')) # Output layer with 1 neuron\n",
        "\n",
        "# define callbacks to monitor the training process\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
        "checkpoint = ModelCheckpoint('model_weights.h5', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(train_X, train_y, epochs=100, batch_size=64, validation_split=0.2, callbacks=[early_stop, checkpoint])\n",
        "\n",
        "model.save(\"feed_forward_NN.h5\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HgjZ03ZUXEk",
        "outputId": "bd90f2ed-3a88-4f54-8057-8aab96fc1688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "30/34 [=========================>....] - ETA: 0s - loss: 468395107549184.0000\n",
            "Epoch 1: val_loss improved from inf to 555376315465728.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 2s 16ms/step - loss: 458209894596608.0000 - val_loss: 555376315465728.0000\n",
            "Epoch 2/100\n",
            "28/34 [=======================>......] - ETA: 0s - loss: 440174186070016.0000\n",
            "Epoch 2: val_loss improved from 555376315465728.00000 to 406125832830976.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 407128976130048.0000 - val_loss: 406125832830976.0000\n",
            "Epoch 3/100\n",
            "31/34 [==========================>...] - ETA: 0s - loss: 311000964268032.0000\n",
            "Epoch 3: val_loss improved from 406125832830976.00000 to 376631352885248.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 305063541080064.0000 - val_loss: 376631352885248.0000\n",
            "Epoch 4/100\n",
            "32/34 [===========================>..] - ETA: 0s - loss: 305001800925184.0000\n",
            "Epoch 4: val_loss did not improve from 376631352885248.00000\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 302034918047744.0000 - val_loss: 379208064827392.0000\n",
            "Epoch 5/100\n",
            "32/34 [===========================>..] - ETA: 0s - loss: 312404143505408.0000\n",
            "Epoch 5: val_loss did not improve from 376631352885248.00000\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 301690213367808.0000 - val_loss: 377188356456448.0000\n",
            "Epoch 6/100\n",
            "30/34 [=========================>....] - ETA: 0s - loss: 301327288631296.0000\n",
            "Epoch 6: val_loss did not improve from 376631352885248.00000\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 301000099364864.0000 - val_loss: 377743078326272.0000\n",
            "Epoch 7/100\n",
            "30/34 [=========================>....] - ETA: 0s - loss: 289529013469184.0000\n",
            "Epoch 7: val_loss improved from 376631352885248.00000 to 375190089367552.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 300224723550208.0000 - val_loss: 375190089367552.0000\n",
            "Epoch 8/100\n",
            "30/34 [=========================>....] - ETA: 0s - loss: 308560651091968.0000\n",
            "Epoch 8: val_loss did not improve from 375190089367552.00000\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 299591081656320.0000 - val_loss: 377824481378304.0000\n",
            "Epoch 9/100\n",
            "29/34 [========================>.....] - ETA: 0s - loss: 293010487115776.0000\n",
            "Epoch 9: val_loss improved from 375190089367552.00000 to 372292295065600.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 301336381882368.0000 - val_loss: 372292295065600.0000\n",
            "Epoch 10/100\n",
            "29/34 [========================>.....] - ETA: 0s - loss: 313685721481216.0000\n",
            "Epoch 10: val_loss did not improve from 372292295065600.00000\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 298240951975936.0000 - val_loss: 372390676660224.0000\n",
            "Epoch 11/100\n",
            "30/34 [=========================>....] - ETA: 0s - loss: 302424786993152.0000\n",
            "Epoch 11: val_loss improved from 372292295065600.00000 to 370152394719232.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 297354141892608.0000 - val_loss: 370152394719232.0000\n",
            "Epoch 12/100\n",
            "32/34 [===========================>..] - ETA: 0s - loss: 288868091822080.0000\n",
            "Epoch 12: val_loss did not improve from 370152394719232.00000\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 296936858976256.0000 - val_loss: 371122755338240.0000\n",
            "Epoch 13/100\n",
            "31/34 [==========================>...] - ETA: 0s - loss: 272644205182976.0000\n",
            "Epoch 13: val_loss improved from 370152394719232.00000 to 367876263378944.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 295169010171904.0000 - val_loss: 367876263378944.0000\n",
            "Epoch 14/100\n",
            "31/34 [==========================>...] - ETA: 0s - loss: 299884951371776.0000\n",
            "Epoch 14: val_loss improved from 367876263378944.00000 to 367871666421760.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 294964328136704.0000 - val_loss: 367871666421760.0000\n",
            "Epoch 15/100\n",
            "29/34 [========================>.....] - ETA: 0s - loss: 282430204280832.0000\n",
            "Epoch 15: val_loss improved from 367871666421760.00000 to 367227320664064.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 294465474396160.0000 - val_loss: 367227320664064.0000\n",
            "Epoch 16/100\n",
            "34/34 [==============================] - ETA: 0s - loss: 292624376266752.0000\n",
            "Epoch 16: val_loss improved from 367227320664064.00000 to 364168934850560.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 292624376266752.0000 - val_loss: 364168934850560.0000\n",
            "Epoch 17/100\n",
            "28/34 [=======================>......] - ETA: 0s - loss: 285853393879040.0000\n",
            "Epoch 17: val_loss improved from 364168934850560.00000 to 363647431868416.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 290463001083904.0000 - val_loss: 363647431868416.0000\n",
            "Epoch 18/100\n",
            "33/34 [============================>.] - ETA: 0s - loss: 287155406503936.0000\n",
            "Epoch 18: val_loss improved from 363647431868416.00000 to 359290489536512.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 289319096942592.0000 - val_loss: 359290489536512.0000\n",
            "Epoch 19/100\n",
            "32/34 [===========================>..] - ETA: 0s - loss: 274717416095744.0000\n",
            "Epoch 19: val_loss improved from 359290489536512.00000 to 358458373177344.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 286398418518016.0000 - val_loss: 358458373177344.0000\n",
            "Epoch 20/100\n",
            "31/34 [==========================>...] - ETA: 0s - loss: 290880317554688.0000\n",
            "Epoch 20: val_loss improved from 358458373177344.00000 to 357550423801856.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 13ms/step - loss: 282943251546112.0000 - val_loss: 357550423801856.0000\n",
            "Epoch 21/100\n",
            "29/34 [========================>.....] - ETA: 0s - loss: 289730709159936.0000\n",
            "Epoch 21: val_loss improved from 357550423801856.00000 to 346924104286208.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 278266820689920.0000 - val_loss: 346924104286208.0000\n",
            "Epoch 22/100\n",
            "31/34 [==========================>...] - ETA: 0s - loss: 266596270473216.0000\n",
            "Epoch 22: val_loss improved from 346924104286208.00000 to 334801861083136.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 269822277451776.0000 - val_loss: 334801861083136.0000\n",
            "Epoch 23/100\n",
            "29/34 [========================>.....] - ETA: 0s - loss: 263526979469312.0000\n",
            "Epoch 23: val_loss improved from 334801861083136.00000 to 317303661002752.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 257597072474112.0000 - val_loss: 317303661002752.0000\n",
            "Epoch 24/100\n",
            "31/34 [==========================>...] - ETA: 0s - loss: 245230687223808.0000\n",
            "Epoch 24: val_loss improved from 317303661002752.00000 to 300879437627392.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 241784814829568.0000 - val_loss: 300879437627392.0000\n",
            "Epoch 25/100\n",
            "29/34 [========================>.....] - ETA: 0s - loss: 238296563187712.0000\n",
            "Epoch 25: val_loss improved from 300879437627392.00000 to 296579571384320.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 13ms/step - loss: 229814740975616.0000 - val_loss: 296579571384320.0000\n",
            "Epoch 26/100\n",
            "34/34 [==============================] - ETA: 0s - loss: 221811690176512.0000\n",
            "Epoch 26: val_loss improved from 296579571384320.00000 to 283212089655296.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 221811690176512.0000 - val_loss: 283212089655296.0000\n",
            "Epoch 27/100\n",
            "34/34 [==============================] - ETA: 0s - loss: 218428765896704.0000\n",
            "Epoch 27: val_loss improved from 283212089655296.00000 to 278383741108224.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 218428765896704.0000 - val_loss: 278383741108224.0000\n",
            "Epoch 28/100\n",
            "30/34 [=========================>....] - ETA: 0s - loss: 218341188829184.0000\n",
            "Epoch 28: val_loss did not improve from 278383741108224.00000\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 216501147664384.0000 - val_loss: 295389664116736.0000\n",
            "Epoch 29/100\n",
            "30/34 [=========================>....] - ETA: 0s - loss: 206831062351872.0000\n",
            "Epoch 29: val_loss improved from 278383741108224.00000 to 276265198157824.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 210940658188288.0000 - val_loss: 276265198157824.0000\n",
            "Epoch 30/100\n",
            "34/34 [==============================] - ETA: 0s - loss: 211164667576320.0000\n",
            "Epoch 30: val_loss improved from 276265198157824.00000 to 270706554175488.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 211164667576320.0000 - val_loss: 270706554175488.0000\n",
            "Epoch 31/100\n",
            "28/34 [=======================>......] - ETA: 0s - loss: 207248177496064.0000\n",
            "Epoch 31: val_loss improved from 270706554175488.00000 to 265089005387776.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 208463552577536.0000 - val_loss: 265089005387776.0000\n",
            "Epoch 32/100\n",
            "34/34 [==============================] - ETA: 0s - loss: 205874928812032.0000\n",
            "Epoch 32: val_loss did not improve from 265089005387776.00000\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 205874928812032.0000 - val_loss: 275489637793792.0000\n",
            "Epoch 33/100\n",
            "26/34 [=====================>........] - ETA: 0s - loss: 197857986478080.0000\n",
            "Epoch 33: val_loss improved from 265089005387776.00000 to 261097856696320.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 205077977497600.0000 - val_loss: 261097856696320.0000\n",
            "Epoch 34/100\n",
            "32/34 [===========================>..] - ETA: 0s - loss: 199389461086208.0000\n",
            "Epoch 34: val_loss improved from 261097856696320.00000 to 257912148590592.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 201317884624896.0000 - val_loss: 257912148590592.0000\n",
            "Epoch 35/100\n",
            "29/34 [========================>.....] - ETA: 0s - loss: 203790477164544.0000\n",
            "Epoch 35: val_loss did not improve from 257912148590592.00000\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 203890972688384.0000 - val_loss: 265310783406080.0000\n",
            "Epoch 36/100\n",
            "32/34 [===========================>..] - ETA: 0s - loss: 190603753160704.0000\n",
            "Epoch 36: val_loss did not improve from 257912148590592.00000\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 199035881259008.0000 - val_loss: 261677727612928.0000\n",
            "Epoch 37/100\n",
            "30/34 [=========================>....] - ETA: 0s - loss: 200018237587456.0000\n",
            "Epoch 37: val_loss did not improve from 257912148590592.00000\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 197228572442624.0000 - val_loss: 260408782880768.0000\n",
            "Epoch 38/100\n",
            "29/34 [========================>.....] - ETA: 0s - loss: 199346897289216.0000\n",
            "Epoch 38: val_loss improved from 257912148590592.00000 to 252385230323712.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 195361956167680.0000 - val_loss: 252385230323712.0000\n",
            "Epoch 39/100\n",
            "30/34 [=========================>....] - ETA: 0s - loss: 199204727160832.0000\n",
            "Epoch 39: val_loss improved from 252385230323712.00000 to 252100990730240.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 194795020484608.0000 - val_loss: 252100990730240.0000\n",
            "Epoch 40/100\n",
            "30/34 [=========================>....] - ETA: 0s - loss: 182918026625024.0000\n",
            "Epoch 40: val_loss improved from 252100990730240.00000 to 246659585933312.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 193822428495872.0000 - val_loss: 246659585933312.0000\n",
            "Epoch 41/100\n",
            "32/34 [===========================>..] - ETA: 0s - loss: 194137554944000.0000\n",
            "Epoch 41: val_loss improved from 246659585933312.00000 to 245146616594432.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 192004298375168.0000 - val_loss: 245146616594432.0000\n",
            "Epoch 42/100\n",
            "31/34 [==========================>...] - ETA: 0s - loss: 189339908702208.0000\n",
            "Epoch 42: val_loss improved from 245146616594432.00000 to 242902764617728.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 191631391195136.0000 - val_loss: 242902764617728.0000\n",
            "Epoch 43/100\n",
            "30/34 [=========================>....] - ETA: 0s - loss: 190896247144448.0000\n",
            "Epoch 43: val_loss did not improve from 242902764617728.00000\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 197449511600128.0000 - val_loss: 264467006881792.0000\n",
            "Epoch 44/100\n",
            "31/34 [==========================>...] - ETA: 0s - loss: 190073945456640.0000\n",
            "Epoch 44: val_loss improved from 242902764617728.00000 to 240229013258240.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 189428056195072.0000 - val_loss: 240229013258240.0000\n",
            "Epoch 45/100\n",
            "29/34 [========================>.....] - ETA: 0s - loss: 196724937195520.0000\n",
            "Epoch 45: val_loss did not improve from 240229013258240.00000\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 188647680770048.0000 - val_loss: 247730626625536.0000\n",
            "Epoch 46/100\n",
            "29/34 [========================>.....] - ETA: 0s - loss: 181199989374976.0000\n",
            "Epoch 46: val_loss improved from 240229013258240.00000 to 238985620226048.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 189024379600896.0000 - val_loss: 238985620226048.0000\n",
            "Epoch 47/100\n",
            "29/34 [========================>.....] - ETA: 0s - loss: 187833230819328.0000\n",
            "Epoch 47: val_loss improved from 238985620226048.00000 to 237531035598848.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 185923195109376.0000 - val_loss: 237531035598848.0000\n",
            "Epoch 48/100\n",
            "32/34 [===========================>..] - ETA: 0s - loss: 188446907826176.0000\n",
            "Epoch 48: val_loss did not improve from 237531035598848.00000\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 184650660052992.0000 - val_loss: 237817640779776.0000\n",
            "Epoch 49/100\n",
            "31/34 [==========================>...] - ETA: 0s - loss: 181355161845760.0000\n",
            "Epoch 49: val_loss did not improve from 237531035598848.00000\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 184365782925312.0000 - val_loss: 244987870576640.0000\n",
            "Epoch 50/100\n",
            "30/34 [=========================>....] - ETA: 0s - loss: 179302855344128.0000\n",
            "Epoch 50: val_loss improved from 237531035598848.00000 to 232690221580288.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 183205235785728.0000 - val_loss: 232690221580288.0000\n",
            "Epoch 51/100\n",
            "29/34 [========================>.....] - ETA: 0s - loss: 186065885331456.0000\n",
            "Epoch 51: val_loss did not improve from 232690221580288.00000\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 182294937600000.0000 - val_loss: 235012322492416.0000\n",
            "Epoch 52/100\n",
            "27/34 [======================>.......] - ETA: 0s - loss: 167778887663616.0000\n",
            "Epoch 52: val_loss improved from 232690221580288.00000 to 231714794242048.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 182687809667072.0000 - val_loss: 231714794242048.0000\n",
            "Epoch 53/100\n",
            "29/34 [========================>.....] - ETA: 0s - loss: 183070481186816.0000\n",
            "Epoch 53: val_loss did not improve from 231714794242048.00000\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 182164259864576.0000 - val_loss: 246374171934720.0000\n",
            "Epoch 54/100\n",
            "27/34 [======================>.......] - ETA: 0s - loss: 175049982083072.0000\n",
            "Epoch 54: val_loss improved from 231714794242048.00000 to 229668410097664.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 180151027499008.0000 - val_loss: 229668410097664.0000\n",
            "Epoch 55/100\n",
            "31/34 [==========================>...] - ETA: 0s - loss: 177601695973376.0000\n",
            "Epoch 55: val_loss did not improve from 229668410097664.00000\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 179417611501568.0000 - val_loss: 232691949633536.0000\n",
            "Epoch 56/100\n",
            "32/34 [===========================>..] - ETA: 0s - loss: 179145636052992.0000\n",
            "Epoch 56: val_loss improved from 229668410097664.00000 to 227790385315840.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 179037305569280.0000 - val_loss: 227790385315840.0000\n",
            "Epoch 57/100\n",
            "34/34 [==============================] - ETA: 0s - loss: 178337678884864.0000\n",
            "Epoch 57: val_loss did not improve from 227790385315840.00000\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 178337678884864.0000 - val_loss: 233265294213120.0000\n",
            "Epoch 58/100\n",
            "30/34 [=========================>....] - ETA: 0s - loss: 180343780933632.0000\n",
            "Epoch 58: val_loss did not improve from 227790385315840.00000\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 183575223730176.0000 - val_loss: 230519853809664.0000\n",
            "Epoch 59/100\n",
            "32/34 [===========================>..] - ETA: 0s - loss: 180084438728704.0000\n",
            "Epoch 59: val_loss did not improve from 227790385315840.00000\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 177691152089088.0000 - val_loss: 234884412997632.0000\n",
            "Epoch 60/100\n",
            "33/34 [============================>.] - ETA: 0s - loss: 177418170007552.0000\n",
            "Epoch 60: val_loss improved from 227790385315840.00000 to 225737709715456.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 178526389010432.0000 - val_loss: 225737709715456.0000\n",
            "Epoch 61/100\n",
            "32/34 [===========================>..] - ETA: 0s - loss: 173478997131264.0000\n",
            "Epoch 61: val_loss improved from 225737709715456.00000 to 223946809016320.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 175741840916480.0000 - val_loss: 223946809016320.0000\n",
            "Epoch 62/100\n",
            "31/34 [==========================>...] - ETA: 0s - loss: 176300387991552.0000\n",
            "Epoch 62: val_loss did not improve from 223946809016320.00000\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 176704718897152.0000 - val_loss: 228921438109696.0000\n",
            "Epoch 63/100\n",
            "33/34 [============================>.] - ETA: 0s - loss: 176395061821440.0000\n",
            "Epoch 63: val_loss did not improve from 223946809016320.00000\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 174613925462016.0000 - val_loss: 229343166988288.0000\n",
            "Epoch 64/100\n",
            "31/34 [==========================>...] - ETA: 0s - loss: 178848930988032.0000\n",
            "Epoch 64: val_loss improved from 223946809016320.00000 to 222494187323392.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 177155875012608.0000 - val_loss: 222494187323392.0000\n",
            "Epoch 65/100\n",
            "32/34 [===========================>..] - ETA: 0s - loss: 180101148835840.0000\n",
            "Epoch 65: val_loss improved from 222494187323392.00000 to 221403752169472.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 174720930545664.0000 - val_loss: 221403752169472.0000\n",
            "Epoch 66/100\n",
            "28/34 [=======================>......] - ETA: 0s - loss: 186031777251328.0000\n",
            "Epoch 66: val_loss did not improve from 221403752169472.00000\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 175814284935168.0000 - val_loss: 221878681600000.0000\n",
            "Epoch 67/100\n",
            "30/34 [=========================>....] - ETA: 0s - loss: 176447071191040.0000\n",
            "Epoch 67: val_loss improved from 221403752169472.00000 to 220348465283072.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 173832425963520.0000 - val_loss: 220348465283072.0000\n",
            "Epoch 68/100\n",
            "28/34 [=======================>......] - ETA: 0s - loss: 175381936078848.0000\n",
            "Epoch 68: val_loss did not improve from 220348465283072.00000\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 171832514707456.0000 - val_loss: 227744063422464.0000\n",
            "Epoch 69/100\n",
            "30/34 [=========================>....] - ETA: 0s - loss: 174474288693248.0000\n",
            "Epoch 69: val_loss did not improve from 220348465283072.00000\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 172562592038912.0000 - val_loss: 220985093521408.0000\n",
            "Epoch 70/100\n",
            "34/34 [==============================] - ETA: 0s - loss: 171739686371328.0000\n",
            "Epoch 70: val_loss improved from 220348465283072.00000 to 216572719267840.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 171739686371328.0000 - val_loss: 216572719267840.0000\n",
            "Epoch 71/100\n",
            "28/34 [=======================>......] - ETA: 0s - loss: 169849699434496.0000\n",
            "Epoch 71: val_loss did not improve from 216572719267840.00000\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 169179315437568.0000 - val_loss: 224187041972224.0000\n",
            "Epoch 72/100\n",
            "31/34 [==========================>...] - ETA: 0s - loss: 178103670276096.0000\n",
            "Epoch 72: val_loss did not improve from 216572719267840.00000\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 175167741362176.0000 - val_loss: 224904502837248.0000\n",
            "Epoch 73/100\n",
            "32/34 [===========================>..] - ETA: 0s - loss: 169075212812288.0000\n",
            "Epoch 73: val_loss did not improve from 216572719267840.00000\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 170277551996928.0000 - val_loss: 218250776412160.0000\n",
            "Epoch 74/100\n",
            "29/34 [========================>.....] - ETA: 0s - loss: 172425002090496.0000\n",
            "Epoch 74: val_loss improved from 216572719267840.00000 to 213399610851328.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 168374512386048.0000 - val_loss: 213399610851328.0000\n",
            "Epoch 75/100\n",
            "30/34 [=========================>....] - ETA: 0s - loss: 155635169624064.0000\n",
            "Epoch 75: val_loss did not improve from 213399610851328.00000\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 168358657916928.0000 - val_loss: 215751843643392.0000\n",
            "Epoch 76/100\n",
            "28/34 [=======================>......] - ETA: 0s - loss: 156611402268672.0000\n",
            "Epoch 76: val_loss did not improve from 213399610851328.00000\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 170680070963200.0000 - val_loss: 214005838774272.0000\n",
            "Epoch 77/100\n",
            "28/34 [=======================>......] - ETA: 0s - loss: 171707994210304.0000\n",
            "Epoch 77: val_loss did not improve from 213399610851328.00000\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 169327575695360.0000 - val_loss: 216962521104384.0000\n",
            "Epoch 78/100\n",
            "34/34 [==============================] - ETA: 0s - loss: 167658276257792.0000\n",
            "Epoch 78: val_loss improved from 213399610851328.00000 to 211261774102528.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 167658276257792.0000 - val_loss: 211261774102528.0000\n",
            "Epoch 79/100\n",
            "27/34 [======================>.......] - ETA: 0s - loss: 174697643769856.0000\n",
            "Epoch 79: val_loss did not improve from 211261774102528.00000\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 166160087646208.0000 - val_loss: 217007433711616.0000\n",
            "Epoch 80/100\n",
            "28/34 [=======================>......] - ETA: 0s - loss: 172602672807936.0000\n",
            "Epoch 80: val_loss did not improve from 211261774102528.00000\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 166824297627648.0000 - val_loss: 213902306574336.0000\n",
            "Epoch 81/100\n",
            "27/34 [======================>.......] - ETA: 0s - loss: 166096569106432.0000\n",
            "Epoch 81: val_loss did not improve from 211261774102528.00000\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 170781103357952.0000 - val_loss: 216149497217024.0000\n",
            "Epoch 82/100\n",
            "33/34 [============================>.] - ETA: 0s - loss: 174403572727808.0000\n",
            "Epoch 82: val_loss did not improve from 211261774102528.00000\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 172477430890496.0000 - val_loss: 220868206657536.0000\n",
            "Epoch 83/100\n",
            "29/34 [========================>.....] - ETA: 0s - loss: 164833076969472.0000\n",
            "Epoch 83: val_loss improved from 211261774102528.00000 to 209371803942912.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 171428737449984.0000 - val_loss: 209371803942912.0000\n",
            "Epoch 84/100\n",
            "29/34 [========================>.....] - ETA: 0s - loss: 161465872941056.0000\n",
            "Epoch 84: val_loss improved from 209371803942912.00000 to 208124770254848.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 165180298231808.0000 - val_loss: 208124770254848.0000\n",
            "Epoch 85/100\n",
            "32/34 [===========================>..] - ETA: 0s - loss: 173527634280448.0000\n",
            "Epoch 85: val_loss did not improve from 208124770254848.00000\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 171657444458496.0000 - val_loss: 220044093030400.0000\n",
            "Epoch 86/100\n",
            "30/34 [=========================>....] - ETA: 0s - loss: 166531770089472.0000\n",
            "Epoch 86: val_loss did not improve from 208124770254848.00000\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 166152856666112.0000 - val_loss: 208232899411968.0000\n",
            "Epoch 87/100\n",
            "33/34 [============================>.] - ETA: 0s - loss: 162348404834304.0000\n",
            "Epoch 87: val_loss improved from 208124770254848.00000 to 206209063845888.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 163980693733376.0000 - val_loss: 206209063845888.0000\n",
            "Epoch 88/100\n",
            "30/34 [=========================>....] - ETA: 0s - loss: 174095744368640.0000\n",
            "Epoch 88: val_loss did not improve from 206209063845888.00000\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 166067561299968.0000 - val_loss: 213921046724608.0000\n",
            "Epoch 89/100\n",
            "29/34 [========================>.....] - ETA: 0s - loss: 173350399770624.0000\n",
            "Epoch 89: val_loss did not improve from 206209063845888.00000\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 165474637709312.0000 - val_loss: 208598894379008.0000\n",
            "Epoch 90/100\n",
            "29/34 [========================>.....] - ETA: 0s - loss: 173080202706944.0000\n",
            "Epoch 90: val_loss did not improve from 206209063845888.00000\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 164722179571712.0000 - val_loss: 217999336275968.0000\n",
            "Epoch 91/100\n",
            "34/34 [==============================] - ETA: 0s - loss: 162900895334400.0000\n",
            "Epoch 91: val_loss improved from 206209063845888.00000 to 204186033586176.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 162900895334400.0000 - val_loss: 204186033586176.0000\n",
            "Epoch 92/100\n",
            "34/34 [==============================] - ETA: 0s - loss: 161195759763456.0000\n",
            "Epoch 92: val_loss did not improve from 204186033586176.00000\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 161195759763456.0000 - val_loss: 215711913869312.0000\n",
            "Epoch 93/100\n",
            "31/34 [==========================>...] - ETA: 0s - loss: 163344652697600.0000\n",
            "Epoch 93: val_loss did not improve from 204186033586176.00000\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 162470693961728.0000 - val_loss: 206810292158464.0000\n",
            "Epoch 94/100\n",
            "31/34 [==========================>...] - ETA: 0s - loss: 161530163232768.0000\n",
            "Epoch 94: val_loss improved from 204186033586176.00000 to 203347843874816.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 161615609593856.0000 - val_loss: 203347843874816.0000\n",
            "Epoch 95/100\n",
            "31/34 [==========================>...] - ETA: 0s - loss: 160014677311488.0000\n",
            "Epoch 95: val_loss did not improve from 203347843874816.00000\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 163653319917568.0000 - val_loss: 204140030459904.0000\n",
            "Epoch 96/100\n",
            "33/34 [============================>.] - ETA: 0s - loss: 157029071060992.0000\n",
            "Epoch 96: val_loss did not improve from 203347843874816.00000\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 164040336736256.0000 - val_loss: 209620190625792.0000\n",
            "Epoch 97/100\n",
            "34/34 [==============================] - ETA: 0s - loss: 162247104004096.0000\n",
            "Epoch 97: val_loss improved from 203347843874816.00000 to 201086761697280.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 1s 19ms/step - loss: 162247104004096.0000 - val_loss: 201086761697280.0000\n",
            "Epoch 98/100\n",
            "31/34 [==========================>...] - ETA: 0s - loss: 150968452775936.0000\n",
            "Epoch 98: val_loss improved from 201086761697280.00000 to 200388242309120.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 160791479189504.0000 - val_loss: 200388242309120.0000\n",
            "Epoch 99/100\n",
            "30/34 [=========================>....] - ETA: 0s - loss: 166517710782464.0000\n",
            "Epoch 99: val_loss did not improve from 200388242309120.00000\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 162166959243264.0000 - val_loss: 213072102817792.0000\n",
            "Epoch 100/100\n",
            "30/34 [=========================>....] - ETA: 0s - loss: 163667312115712.0000\n",
            "Epoch 100: val_loss did not improve from 200388242309120.00000\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 159713308180480.0000 - val_loss: 202918296813568.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "row = test_X.iloc[5]\n",
        "row2 = test_y.iloc[5]\n",
        "# make prediction for the selected row\n",
        "prediction = model.predict(np.array([row]))\n",
        "print(row2)\n",
        "# print the prediction\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DIxTrSqCRoe",
        "outputId": "1cb45aaa-ff85-4737-dbc0-a7b622e6c7f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n",
            "8000000.0\n",
            "[[7014145.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "QJiu8vXVC1gP",
        "outputId": "6687537e-7d25-4931-de45-f1d3591b85f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   minutes_played_overall  minutes_played_home  minutes_played_away  \\\n",
              "0               -0.060716            -0.003595            -0.117620   \n",
              "1               -0.041368            -0.145131             0.064552   \n",
              "2               -1.233552            -1.210274            -1.236417   \n",
              "3               -1.237237            -1.217532            -1.236417   \n",
              "4                0.993270             0.816583             1.155747   \n",
              "\n",
              "   appearances_overall  appearances_home  appearances_away  goals_overall  \\\n",
              "0            -0.279406         -0.196729         -0.356941      -0.257054   \n",
              "1            -0.200715         -0.196729         -0.200484      -0.534581   \n",
              "2            -1.381076         -1.281678         -1.452141      -0.534581   \n",
              "3            -1.381076         -1.436670         -1.295684      -0.534581   \n",
              "4             0.979646          0.578234          1.364086       0.853053   \n",
              "\n",
              "   goals_home  goals_away  assists_overall  ...  Aggression  \\\n",
              "0   -0.028932   -0.480840        -0.608484  ...    1.316970   \n",
              "1   -0.510339   -0.480840        -0.608484  ...    1.316970   \n",
              "2   -0.510339   -0.480840        -0.608484  ...   -1.783763   \n",
              "3   -0.510339   -0.480840        -0.608484  ...   -1.783763   \n",
              "4    0.933884    0.629474        -0.154288  ...    0.978708   \n",
              "\n",
              "   rank_in_club_top_scorer  Year  GK Positioning  penalty_goals  GK Handling  \\\n",
              "0                -0.308979  2017       -0.087152       -0.21616    -0.520990   \n",
              "1                 0.386336  2013       -0.087152       -0.21616    -0.520990   \n",
              "2                 1.197537  2022       -0.246409       -0.21616    -0.574494   \n",
              "3                -1.467838  2021       -0.246409       -0.21616    -0.574494   \n",
              "4                -1.004294  2015       -0.564921       -0.21616    -0.574494   \n",
              "\n",
              "   Defense Awareness/Marking  min_per_match  Standing Tackle  Market Values  \n",
              "0                   1.037086       0.892382         1.077421      5000000.0  \n",
              "1                   1.037086       0.761109         1.077421      4500000.0  \n",
              "2                  -0.863667      -1.765896        -0.906437      3000000.0  \n",
              "3                  -0.863667      -1.897169        -0.906437       500000.0  \n",
              "4                   0.821092       0.662654         1.161840      1500000.0  \n",
              "\n",
              "[5 rows x 40 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a21cd0ee-7b5f-40c7-916d-c3e4d185638e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>minutes_played_overall</th>\n",
              "      <th>minutes_played_home</th>\n",
              "      <th>minutes_played_away</th>\n",
              "      <th>appearances_overall</th>\n",
              "      <th>appearances_home</th>\n",
              "      <th>appearances_away</th>\n",
              "      <th>goals_overall</th>\n",
              "      <th>goals_home</th>\n",
              "      <th>goals_away</th>\n",
              "      <th>assists_overall</th>\n",
              "      <th>...</th>\n",
              "      <th>Aggression</th>\n",
              "      <th>rank_in_club_top_scorer</th>\n",
              "      <th>Year</th>\n",
              "      <th>GK Positioning</th>\n",
              "      <th>penalty_goals</th>\n",
              "      <th>GK Handling</th>\n",
              "      <th>Defense Awareness/Marking</th>\n",
              "      <th>min_per_match</th>\n",
              "      <th>Standing Tackle</th>\n",
              "      <th>Market Values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.060716</td>\n",
              "      <td>-0.003595</td>\n",
              "      <td>-0.117620</td>\n",
              "      <td>-0.279406</td>\n",
              "      <td>-0.196729</td>\n",
              "      <td>-0.356941</td>\n",
              "      <td>-0.257054</td>\n",
              "      <td>-0.028932</td>\n",
              "      <td>-0.480840</td>\n",
              "      <td>-0.608484</td>\n",
              "      <td>...</td>\n",
              "      <td>1.316970</td>\n",
              "      <td>-0.308979</td>\n",
              "      <td>2017</td>\n",
              "      <td>-0.087152</td>\n",
              "      <td>-0.21616</td>\n",
              "      <td>-0.520990</td>\n",
              "      <td>1.037086</td>\n",
              "      <td>0.892382</td>\n",
              "      <td>1.077421</td>\n",
              "      <td>5000000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.041368</td>\n",
              "      <td>-0.145131</td>\n",
              "      <td>0.064552</td>\n",
              "      <td>-0.200715</td>\n",
              "      <td>-0.196729</td>\n",
              "      <td>-0.200484</td>\n",
              "      <td>-0.534581</td>\n",
              "      <td>-0.510339</td>\n",
              "      <td>-0.480840</td>\n",
              "      <td>-0.608484</td>\n",
              "      <td>...</td>\n",
              "      <td>1.316970</td>\n",
              "      <td>0.386336</td>\n",
              "      <td>2013</td>\n",
              "      <td>-0.087152</td>\n",
              "      <td>-0.21616</td>\n",
              "      <td>-0.520990</td>\n",
              "      <td>1.037086</td>\n",
              "      <td>0.761109</td>\n",
              "      <td>1.077421</td>\n",
              "      <td>4500000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.233552</td>\n",
              "      <td>-1.210274</td>\n",
              "      <td>-1.236417</td>\n",
              "      <td>-1.381076</td>\n",
              "      <td>-1.281678</td>\n",
              "      <td>-1.452141</td>\n",
              "      <td>-0.534581</td>\n",
              "      <td>-0.510339</td>\n",
              "      <td>-0.480840</td>\n",
              "      <td>-0.608484</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.783763</td>\n",
              "      <td>1.197537</td>\n",
              "      <td>2022</td>\n",
              "      <td>-0.246409</td>\n",
              "      <td>-0.21616</td>\n",
              "      <td>-0.574494</td>\n",
              "      <td>-0.863667</td>\n",
              "      <td>-1.765896</td>\n",
              "      <td>-0.906437</td>\n",
              "      <td>3000000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.237237</td>\n",
              "      <td>-1.217532</td>\n",
              "      <td>-1.236417</td>\n",
              "      <td>-1.381076</td>\n",
              "      <td>-1.436670</td>\n",
              "      <td>-1.295684</td>\n",
              "      <td>-0.534581</td>\n",
              "      <td>-0.510339</td>\n",
              "      <td>-0.480840</td>\n",
              "      <td>-0.608484</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.783763</td>\n",
              "      <td>-1.467838</td>\n",
              "      <td>2021</td>\n",
              "      <td>-0.246409</td>\n",
              "      <td>-0.21616</td>\n",
              "      <td>-0.574494</td>\n",
              "      <td>-0.863667</td>\n",
              "      <td>-1.897169</td>\n",
              "      <td>-0.906437</td>\n",
              "      <td>500000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.993270</td>\n",
              "      <td>0.816583</td>\n",
              "      <td>1.155747</td>\n",
              "      <td>0.979646</td>\n",
              "      <td>0.578234</td>\n",
              "      <td>1.364086</td>\n",
              "      <td>0.853053</td>\n",
              "      <td>0.933884</td>\n",
              "      <td>0.629474</td>\n",
              "      <td>-0.154288</td>\n",
              "      <td>...</td>\n",
              "      <td>0.978708</td>\n",
              "      <td>-1.004294</td>\n",
              "      <td>2015</td>\n",
              "      <td>-0.564921</td>\n",
              "      <td>-0.21616</td>\n",
              "      <td>-0.574494</td>\n",
              "      <td>0.821092</td>\n",
              "      <td>0.662654</td>\n",
              "      <td>1.161840</td>\n",
              "      <td>1500000.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  40 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a21cd0ee-7b5f-40c7-916d-c3e4d185638e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a21cd0ee-7b5f-40c7-916d-c3e4d185638e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a21cd0ee-7b5f-40c7-916d-c3e4d185638e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "row =df.iloc[5]\n",
        "row = row.drop('Market Values')"
      ],
      "metadata": {
        "id": "uPeWSlDu5KKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make prediction for the selected row\n",
        "prediction = model.predict(np.array([row]))\n",
        "\n",
        "# print the prediction\n",
        "print(prediction)"
      ],
      "metadata": {
        "id": "VpZxQ_rL5HIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a013eb8-c371-48b0-9c26-4525b7ee1028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 30ms/step\n",
            "[[12453427.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[5]['Market Values']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOQBWUyNt8Kz",
        "outputId": "48896518-a5c8-4452-94fd-8f6c736d5a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3500000.0"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM**"
      ],
      "metadata": {
        "id": "0irLpdkRZtYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/Feature_selected.csv\")\n",
        "# Loading the dataset\n",
        "X =df.drop([\"Market Values\"],axis=1)\n",
        "y =df[\"Market Values\"]   \n",
        " \n",
        "# Splitting\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y,\n",
        "                      test_size = 0.3, random_state = 123)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Convert DataFrame to NumPy array\n",
        "# train_X = train_X.values\n",
        "\n",
        "# # Reshape train_X to 3D shape\n",
        "# train_X = train_X.reshape((train_X.shape[0], train_X.shape[1], 1))\n",
        "\n",
        "\n",
        "\n",
        "# Reshape train_X to 3D shape\n",
        "train_X = train_X.values.reshape((train_X.shape[0], train_X.shape[1], 1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# define the model architecture\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=64, input_shape=(train_X.shape[1], 1), return_sequences=True))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=64, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=64))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=1))\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# define callbacks to monitor the training process\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
        "checkpoint = ModelCheckpoint('model_weights.h5', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
        "history = model.fit(train_X,train_y, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stop, checkpoint])\n",
        "\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "  json_file.write(model_json)\n",
        "\n",
        "\n",
        "model.save_weights('final_model_weights.h5')\n",
        "\n",
        "#evaluate the model on the test data\n",
        "\n",
        "mse = model.evaluate(test_X, test_y)\n",
        "print('Test MSE: %.3f' % mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDTDSeNOZNNh",
        "outputId": "4a312108-de27-4c0a-df9e-a383a7bd87d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "34/34 [==============================] - ETA: 0s - loss: 459675216642048.0000\n",
            "Epoch 1: val_loss improved from inf to 563768480235520.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 13s 159ms/step - loss: 459675216642048.0000 - val_loss: 563768480235520.0000\n",
            "Epoch 2/10\n",
            "34/34 [==============================] - ETA: 0s - loss: 459675015315456.0000\n",
            "Epoch 2: val_loss did not improve from 563768480235520.00000\n",
            "34/34 [==============================] - 5s 150ms/step - loss: 459675015315456.0000 - val_loss: 563768480235520.0000\n",
            "Epoch 3/10\n",
            "34/34 [==============================] - ETA: 0s - loss: 459674948206592.0000\n",
            "Epoch 3: val_loss improved from 563768480235520.00000 to 563768346017792.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 4s 117ms/step - loss: 459674948206592.0000 - val_loss: 563768346017792.0000\n",
            "Epoch 4/10\n",
            "34/34 [==============================] - ETA: 0s - loss: 459674948206592.0000\n",
            "Epoch 4: val_loss did not improve from 563768346017792.00000\n",
            "34/34 [==============================] - 4s 125ms/step - loss: 459674948206592.0000 - val_loss: 563768346017792.0000\n",
            "Epoch 5/10\n",
            "34/34 [==============================] - ETA: 0s - loss: 459674847543296.0000\n",
            "Epoch 5: val_loss improved from 563768346017792.00000 to 563768211800064.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 6s 184ms/step - loss: 459674847543296.0000 - val_loss: 563768211800064.0000\n",
            "Epoch 6/10\n",
            "34/34 [==============================] - ETA: 0s - loss: 459674914652160.0000\n",
            "Epoch 6: val_loss did not improve from 563768211800064.00000\n",
            "34/34 [==============================] - 4s 117ms/step - loss: 459674914652160.0000 - val_loss: 563768211800064.0000\n",
            "Epoch 7/10\n",
            "34/34 [==============================] - ETA: 0s - loss: 459674780434432.0000\n",
            "Epoch 7: val_loss improved from 563768211800064.00000 to 563768010473472.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 4s 118ms/step - loss: 459674780434432.0000 - val_loss: 563768010473472.0000\n",
            "Epoch 8/10\n",
            "34/34 [==============================] - ETA: 0s - loss: 459674679771136.0000\n",
            "Epoch 8: val_loss did not improve from 563768010473472.00000\n",
            "34/34 [==============================] - 5s 152ms/step - loss: 459674679771136.0000 - val_loss: 563768010473472.0000\n",
            "Epoch 9/10\n",
            "34/34 [==============================] - ETA: 0s - loss: 459674679771136.0000\n",
            "Epoch 9: val_loss did not improve from 563768010473472.00000\n",
            "34/34 [==============================] - 4s 116ms/step - loss: 459674679771136.0000 - val_loss: 563768010473472.0000\n",
            "Epoch 10/10\n",
            "34/34 [==============================] - ETA: 0s - loss: 459674545553408.0000\n",
            "Epoch 10: val_loss improved from 563768010473472.00000 to 563767876255744.00000, saving model to model_weights.h5\n",
            "34/34 [==============================] - 4s 120ms/step - loss: 459674545553408.0000 - val_loss: 563767876255744.0000\n",
            "37/37 [==============================] - 3s 33ms/step - loss: 468437285470208.0000\n",
            "Test MSE: 468437285470208.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auto Encoder"
      ],
      "metadata": {
        "id": "3q7n1CUF-HXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "df=pd.read_csv(\"/content/Feature_selected.csv\")\n",
        "# Loading the dataset\n",
        "X =df.drop([\"Market Values\"],axis=1)\n",
        "y =df[\"Market Values\"]   \n",
        " \n",
        "# Splitting\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y,\n",
        "                      test_size = 0.3, random_state = 123)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(train_X)\n",
        "X_test_scaled = scaler.transform(test_X)\n",
        "\n",
        "# Define the autoencoder model\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "encoding_dim = 10\n",
        "\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "encoder = Dense(encoding_dim, activation='relu')(input_layer)\n",
        "decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
        "\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "\n",
        "# Compile the model\n",
        "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Fit the model to the training data\n",
        "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
        "                epochs=100,\n",
        "                batch_size=64,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_test_scaled, X_test_scaled))\n",
        "\n",
        "# Extract the encoder part of the autoencoder as the feature extractor\n",
        "encoder = Model(inputs=input_layer, outputs=encoder)\n",
        "encoded_X_train = encoder.predict(X_train_scaled)\n",
        "encoded_X_test = encoder.predict(X_test_scaled)\n",
        "\n",
        "# Add a dense layer and an output layer to the encoder\n",
        "model = Sequential()\n",
        "model.add(Dense(32, input_dim=encoding_dim, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Fit the model to the encoded training data\n",
        "model.fit(encoded_X_train, train_y,\n",
        "          epochs=100,\n",
        "          batch_size=64,\n",
        "          shuffle=True,\n",
        "          validation_data=(encoded_X_test,test_y))\n",
        "\n",
        "# Make predictions using the test data\n",
        "predictions = model.predict(encoded_X_test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uwj89EIebXW5",
        "outputId": "2013f2e4-1f22-49ef-e217-3b1ca496ae5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "43/43 [==============================] - 2s 14ms/step - loss: 1.2262 - val_loss: 1.2061\n",
            "Epoch 2/100\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 1.1295 - val_loss: 1.0882\n",
            "Epoch 3/100\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 1.0020 - val_loss: 0.9547\n",
            "Epoch 4/100\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 0.8845 - val_loss: 0.8557\n",
            "Epoch 5/100\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.8065 - val_loss: 0.7954\n",
            "Epoch 6/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 0.7587 - val_loss: 0.7587\n",
            "Epoch 7/100\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 0.7287 - val_loss: 0.7350\n",
            "Epoch 8/100\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 0.7094 - val_loss: 0.7195\n",
            "Epoch 9/100\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.6966 - val_loss: 0.7091\n",
            "Epoch 10/100\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.6877 - val_loss: 0.7015\n",
            "Epoch 11/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6812 - val_loss: 0.6960\n",
            "Epoch 12/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6762 - val_loss: 0.6915\n",
            "Epoch 13/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6721 - val_loss: 0.6877\n",
            "Epoch 14/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6685 - val_loss: 0.6844\n",
            "Epoch 15/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6654 - val_loss: 0.6815\n",
            "Epoch 16/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6625 - val_loss: 0.6788\n",
            "Epoch 17/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6598 - val_loss: 0.6762\n",
            "Epoch 18/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6573 - val_loss: 0.6739\n",
            "Epoch 19/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6551 - val_loss: 0.6717\n",
            "Epoch 20/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6529 - val_loss: 0.6697\n",
            "Epoch 21/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6509 - val_loss: 0.6677\n",
            "Epoch 22/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6490 - val_loss: 0.6658\n",
            "Epoch 23/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6472 - val_loss: 0.6641\n",
            "Epoch 24/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6456 - val_loss: 0.6624\n",
            "Epoch 25/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6440 - val_loss: 0.6610\n",
            "Epoch 26/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6425 - val_loss: 0.6595\n",
            "Epoch 27/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6411 - val_loss: 0.6581\n",
            "Epoch 28/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6397 - val_loss: 0.6568\n",
            "Epoch 29/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6384 - val_loss: 0.6556\n",
            "Epoch 30/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6372 - val_loss: 0.6544\n",
            "Epoch 31/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6361 - val_loss: 0.6533\n",
            "Epoch 32/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6350 - val_loss: 0.6523\n",
            "Epoch 33/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6339 - val_loss: 0.6513\n",
            "Epoch 34/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6329 - val_loss: 0.6504\n",
            "Epoch 35/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6319 - val_loss: 0.6494\n",
            "Epoch 36/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6310 - val_loss: 0.6485\n",
            "Epoch 37/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6301 - val_loss: 0.6476\n",
            "Epoch 38/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6291 - val_loss: 0.6467\n",
            "Epoch 39/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6280 - val_loss: 0.6452\n",
            "Epoch 40/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6266 - val_loss: 0.6443\n",
            "Epoch 41/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6257 - val_loss: 0.6435\n",
            "Epoch 42/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6249 - val_loss: 0.6427\n",
            "Epoch 43/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6240 - val_loss: 0.6420\n",
            "Epoch 44/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6232 - val_loss: 0.6413\n",
            "Epoch 45/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6224 - val_loss: 0.6405\n",
            "Epoch 46/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6216 - val_loss: 0.6399\n",
            "Epoch 47/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 0.6209 - val_loss: 0.6392\n",
            "Epoch 48/100\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 0.6202 - val_loss: 0.6385\n",
            "Epoch 49/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 0.6195 - val_loss: 0.6379\n",
            "Epoch 50/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6188 - val_loss: 0.6372\n",
            "Epoch 51/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6181 - val_loss: 0.6366\n",
            "Epoch 52/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 0.6174 - val_loss: 0.6360\n",
            "Epoch 53/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 0.6168 - val_loss: 0.6354\n",
            "Epoch 54/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6162 - val_loss: 0.6348\n",
            "Epoch 55/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6156 - val_loss: 0.6342\n",
            "Epoch 56/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 0.6150 - val_loss: 0.6336\n",
            "Epoch 57/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 0.6144 - val_loss: 0.6331\n",
            "Epoch 58/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6139 - val_loss: 0.6326\n",
            "Epoch 59/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 0.6133 - val_loss: 0.6320\n",
            "Epoch 60/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6128 - val_loss: 0.6315\n",
            "Epoch 61/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6123 - val_loss: 0.6310\n",
            "Epoch 62/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6118 - val_loss: 0.6305\n",
            "Epoch 63/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6113 - val_loss: 0.6301\n",
            "Epoch 64/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6108 - val_loss: 0.6296\n",
            "Epoch 65/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6103 - val_loss: 0.6292\n",
            "Epoch 66/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6099 - val_loss: 0.6288\n",
            "Epoch 67/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6094 - val_loss: 0.6284\n",
            "Epoch 68/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6090 - val_loss: 0.6279\n",
            "Epoch 69/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 0.6086 - val_loss: 0.6275\n",
            "Epoch 70/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6082 - val_loss: 0.6272\n",
            "Epoch 71/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6078 - val_loss: 0.6268\n",
            "Epoch 72/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6074 - val_loss: 0.6264\n",
            "Epoch 73/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6070 - val_loss: 0.6261\n",
            "Epoch 74/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6067 - val_loss: 0.6257\n",
            "Epoch 75/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6063 - val_loss: 0.6254\n",
            "Epoch 76/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6060 - val_loss: 0.6251\n",
            "Epoch 77/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6057 - val_loss: 0.6248\n",
            "Epoch 78/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6053 - val_loss: 0.6245\n",
            "Epoch 79/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6050 - val_loss: 0.6241\n",
            "Epoch 80/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6047 - val_loss: 0.6238\n",
            "Epoch 81/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6044 - val_loss: 0.6235\n",
            "Epoch 82/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6041 - val_loss: 0.6232\n",
            "Epoch 83/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6038 - val_loss: 0.6229\n",
            "Epoch 84/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6035 - val_loss: 0.6226\n",
            "Epoch 85/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6032 - val_loss: 0.6223\n",
            "Epoch 86/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6029 - val_loss: 0.6220\n",
            "Epoch 87/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6026 - val_loss: 0.6218\n",
            "Epoch 88/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6023 - val_loss: 0.6215\n",
            "Epoch 89/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6020 - val_loss: 0.6212\n",
            "Epoch 90/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 0.6017 - val_loss: 0.6209\n",
            "Epoch 91/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6014 - val_loss: 0.6206\n",
            "Epoch 92/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6012 - val_loss: 0.6204\n",
            "Epoch 93/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6009 - val_loss: 0.6201\n",
            "Epoch 94/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6006 - val_loss: 0.6198\n",
            "Epoch 95/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6004 - val_loss: 0.6196\n",
            "Epoch 96/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.6001 - val_loss: 0.6194\n",
            "Epoch 97/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.5999 - val_loss: 0.6191\n",
            "Epoch 98/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.5996 - val_loss: 0.6188\n",
            "Epoch 99/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.5994 - val_loss: 0.6186\n",
            "Epoch 100/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.5992 - val_loss: 0.6184\n",
            "85/85 [==============================] - 0s 2ms/step\n",
            "37/37 [==============================] - 0s 2ms/step\n",
            "Epoch 1/100\n",
            "43/43 [==============================] - 2s 11ms/step - loss: 480509230579712.0000 - val_loss: 468437889449984.0000\n",
            "Epoch 2/100\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 480509029253120.0000 - val_loss: 468437218361344.0000\n",
            "Epoch 3/100\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 480507049541632.0000 - val_loss: 468433829363712.0000\n",
            "Epoch 4/100\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 480500170883072.0000 - val_loss: 468422555074560.0000\n",
            "Epoch 5/100\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 480480709312512.0000 - val_loss: 468393597599744.0000\n",
            "Epoch 6/100\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 480435477938176.0000 - val_loss: 468334978007040.0000\n",
            "Epoch 7/100\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 480350518116352.0000 - val_loss: 468228140695552.0000\n",
            "Epoch 8/100\n",
            "43/43 [==============================] - 0s 7ms/step - loss: 480209589501952.0000 - val_loss: 468060938960896.0000\n",
            "Epoch 9/100\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 479991385030656.0000 - val_loss: 467804683763712.0000\n",
            "Epoch 10/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 479668088078336.0000 - val_loss: 467445752004608.0000\n",
            "Epoch 11/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 479220908163072.0000 - val_loss: 466952434745344.0000\n",
            "Epoch 12/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 478620283830272.0000 - val_loss: 466284466667520.0000\n",
            "Epoch 13/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 477828332126208.0000 - val_loss: 465437519249408.0000\n",
            "Epoch 14/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 476815055388672.0000 - val_loss: 464363441881088.0000\n",
            "Epoch 15/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 475554012725248.0000 - val_loss: 463025895112704.0000\n",
            "Epoch 16/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 474001684037632.0000 - val_loss: 461390284324864.0000\n",
            "Epoch 17/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 472131628433408.0000 - val_loss: 459459964960768.0000\n",
            "Epoch 18/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 469917069475840.0000 - val_loss: 457163063427072.0000\n",
            "Epoch 19/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 467327841730560.0000 - val_loss: 454563467362304.0000\n",
            "Epoch 20/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 464345926467584.0000 - val_loss: 451506859933696.0000\n",
            "Epoch 21/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 460934346702848.0000 - val_loss: 448069980127232.0000\n",
            "Epoch 22/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 457109913206784.0000 - val_loss: 444304266887168.0000\n",
            "Epoch 23/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 452840380170240.0000 - val_loss: 440071006191616.0000\n",
            "Epoch 24/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 448123063238656.0000 - val_loss: 435341374783488.0000\n",
            "Epoch 25/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 442998529720320.0000 - val_loss: 430199023861760.0000\n",
            "Epoch 26/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 437368398020608.0000 - val_loss: 424761427492864.0000\n",
            "Epoch 27/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 431389937762304.0000 - val_loss: 418801724162048.0000\n",
            "Epoch 28/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 424902255443968.0000 - val_loss: 412686126940160.0000\n",
            "Epoch 29/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 418099463454720.0000 - val_loss: 406085299077120.0000\n",
            "Epoch 30/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 410906265649152.0000 - val_loss: 399152986980352.0000\n",
            "Epoch 31/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 403354773618688.0000 - val_loss: 391943045513216.0000\n",
            "Epoch 32/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 395523907387392.0000 - val_loss: 384572411871232.0000\n",
            "Epoch 33/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 387592646295552.0000 - val_loss: 376927168757760.0000\n",
            "Epoch 34/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 379490492481536.0000 - val_loss: 369463622893568.0000\n",
            "Epoch 35/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 371380151386112.0000 - val_loss: 361908372766720.0000\n",
            "Epoch 36/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 363224310480896.0000 - val_loss: 354430767595520.0000\n",
            "Epoch 37/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 355124471922688.0000 - val_loss: 346982925205504.0000\n",
            "Epoch 38/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 347171601776640.0000 - val_loss: 339762984517632.0000\n",
            "Epoch 39/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 339448109727744.0000 - val_loss: 333066190979072.0000\n",
            "Epoch 40/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 331947251335168.0000 - val_loss: 326363022098432.0000\n",
            "Epoch 41/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 324868239261696.0000 - val_loss: 319939059646464.0000\n",
            "Epoch 42/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 318140407873536.0000 - val_loss: 314174374674432.0000\n",
            "Epoch 43/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 311908007673856.0000 - val_loss: 308686144667648.0000\n",
            "Epoch 44/100\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 306061718323200.0000 - val_loss: 303826389172224.0000\n",
            "Epoch 45/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 300808805548032.0000 - val_loss: 299056593108992.0000\n",
            "Epoch 46/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 295781110120448.0000 - val_loss: 295118913404928.0000\n",
            "Epoch 47/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 291283172065280.0000 - val_loss: 291469399162880.0000\n",
            "Epoch 48/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 287173693669376.0000 - val_loss: 287991415177216.0000\n",
            "Epoch 49/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 283496597684224.0000 - val_loss: 285128651702272.0000\n",
            "Epoch 50/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 280253914152960.0000 - val_loss: 282454430580736.0000\n",
            "Epoch 51/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 277298926321664.0000 - val_loss: 280193331625984.0000\n",
            "Epoch 52/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 274609840586752.0000 - val_loss: 278015917424640.0000\n",
            "Epoch 53/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 272251316338688.0000 - val_loss: 276197015552000.0000\n",
            "Epoch 54/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 270214327435264.0000 - val_loss: 274471076233216.0000\n",
            "Epoch 55/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 268272398237696.0000 - val_loss: 273015417864192.0000\n",
            "Epoch 56/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 266631167082496.0000 - val_loss: 271655758725120.0000\n",
            "Epoch 57/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 265074727976960.0000 - val_loss: 270466170224640.0000\n",
            "Epoch 58/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 263708542500864.0000 - val_loss: 269324732334080.0000\n",
            "Epoch 59/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 262386162335744.0000 - val_loss: 268283169210368.0000\n",
            "Epoch 60/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 261181507895296.0000 - val_loss: 267278817951744.0000\n",
            "Epoch 61/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 260050287329280.0000 - val_loss: 266286697283584.0000\n",
            "Epoch 62/100\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 258965942304768.0000 - val_loss: 265384989032448.0000\n",
            "Epoch 63/100\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 257950719410176.0000 - val_loss: 264503010787328.0000\n",
            "Epoch 64/100\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 256951250321408.0000 - val_loss: 263637910421504.0000\n",
            "Epoch 65/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 256040616591360.0000 - val_loss: 262819819814912.0000\n",
            "Epoch 66/100\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 255210614161408.0000 - val_loss: 262000236036096.0000\n",
            "Epoch 67/100\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 254313905520640.0000 - val_loss: 261216538722304.0000\n",
            "Epoch 68/100\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 253503230443520.0000 - val_loss: 260448293224448.0000\n",
            "Epoch 69/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 252717133987840.0000 - val_loss: 259700264271872.0000\n",
            "Epoch 70/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 251938486616064.0000 - val_loss: 258966747611136.0000\n",
            "Epoch 71/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 251191514628096.0000 - val_loss: 258200918032384.0000\n",
            "Epoch 72/100\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 250441036201984.0000 - val_loss: 257515619090432.0000\n",
            "Epoch 73/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 249699634249728.0000 - val_loss: 256787269812224.0000\n",
            "Epoch 74/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 248978381733888.0000 - val_loss: 256071151124480.0000\n",
            "Epoch 75/100\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 248291136634880.0000 - val_loss: 255370635247616.0000\n",
            "Epoch 76/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 247603488882688.0000 - val_loss: 254694630883328.0000\n",
            "Epoch 77/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 246927216082944.0000 - val_loss: 254021126324224.0000\n",
            "Epoch 78/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 246281427484672.0000 - val_loss: 253332639711232.0000\n",
            "Epoch 79/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 245647517155328.0000 - val_loss: 252661383299072.0000\n",
            "Epoch 80/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 244989263085568.0000 - val_loss: 252005897469952.0000\n",
            "Epoch 81/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 244390064816128.0000 - val_loss: 251350042542080.0000\n",
            "Epoch 82/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 243784205991936.0000 - val_loss: 250717575053312.0000\n",
            "Epoch 83/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 243170310881280.0000 - val_loss: 250081181696000.0000\n",
            "Epoch 84/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 242645452455936.0000 - val_loss: 249462807068672.0000\n",
            "Epoch 85/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 242001777786880.0000 - val_loss: 248836060610560.0000\n",
            "Epoch 86/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 241416135507968.0000 - val_loss: 248257028554752.0000\n",
            "Epoch 87/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 240867520544768.0000 - val_loss: 247640465866752.0000\n",
            "Epoch 88/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 240338635587584.0000 - val_loss: 247013987844096.0000\n",
            "Epoch 89/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 239748782227456.0000 - val_loss: 246462269095936.0000\n",
            "Epoch 90/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 239189798944768.0000 - val_loss: 245878589751296.0000\n",
            "Epoch 91/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 238667373215744.0000 - val_loss: 245315194060800.0000\n",
            "Epoch 92/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 238128975577088.0000 - val_loss: 244755556466688.0000\n",
            "Epoch 93/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 237602389098496.0000 - val_loss: 244196942282752.0000\n",
            "Epoch 94/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 237087966101504.0000 - val_loss: 243630040154112.0000\n",
            "Epoch 95/100\n",
            "43/43 [==============================] - 0s 6ms/step - loss: 236592350363648.0000 - val_loss: 243088152854528.0000\n",
            "Epoch 96/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 236081215700992.0000 - val_loss: 242558026383360.0000\n",
            "Epoch 97/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 235613685022720.0000 - val_loss: 242006727065600.0000\n",
            "Epoch 98/100\n",
            "43/43 [==============================] - 0s 5ms/step - loss: 235117146537984.0000 - val_loss: 241477003247616.0000\n",
            "Epoch 99/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 234648038801408.0000 - val_loss: 240932079271936.0000\n",
            "Epoch 100/100\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 234172186624000.0000 - val_loss: 240404318388224.0000\n",
            "37/37 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Reshape, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/Feature_selected.csv\")\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(df, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the dimensions of the noise input vector\n",
        "noise_dim = 100\n",
        "\n",
        "# Build the generator model\n",
        "generator = Sequential()\n",
        "generator.add(Dense(256, input_dim=noise_dim, activation='relu'))\n",
        "generator.add(Dense(512, activation='relu'))\n",
        "generator.add(Dense(1024, activation='relu'))\n",
        "generator.add(Dense(df.shape[1], activation='linear'))\n",
        "generator.add(Reshape((df.shape[1], 1)))\n",
        "\n",
        "# Build the discriminator model\n",
        "discriminator = Sequential()\n",
        "discriminator.add(Flatten(input_shape=(df.shape[1], 1)))\n",
        "discriminator.add(Dense(512, activation='relu'))\n",
        "discriminator.add(Dense(256, activation='relu'))\n",
        "discriminator.add(Dense(1, activation='sigmoid'))\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
        "\n",
        "# Build the GAN model\n",
        "discriminator.trainable = False\n",
        "gan = Sequential()\n",
        "gan.add(generator)\n",
        "gan.add(discriminator)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
        "\n",
        "# Train the GAN model\n",
        "epochs = 100\n",
        "batch_size = train_data.shape[0] # Set batch size equal to number of samples in train_data\n",
        "for epoch in range(epochs):\n",
        "    # Generate random noise as input to the generator\n",
        "    noise = np.random.normal(0, 1, size=(batch_size, noise_dim))\n",
        "\n",
        "    # Generate fake samples using the generator\n",
        "    fake_data = generator.predict(noise)\n",
        "    train_data_3D = train_data.values.reshape(train_data.shape[0], train_data.shape[1], 1)\n",
        "    \n",
        "    fake_data_3D = fake_data.reshape(fake_data.shape[0], fake_data.shape[1], 1)\n",
        "    data_combined = np.concatenate((train_data_3D, fake_data_3D))\n",
        "\n",
        "    # Labels for fake and real samples\n",
        "    labels_combined = np.concatenate((np.zeros((batch_size, 1)), np.ones((batch_size, 1))))\n",
        "\n",
        "    # Train the discriminator\n",
        "    discriminator.trainable = True\n",
        "    discriminator.train_on_batch(data_combined, labels_combined)\n",
        "\n",
        "    # Train the generator\n",
        "    noise = np.random.normal(0, 1, size=(batch_size, noise_dim))\n",
        "    labels_mislabeled = np.ones((batch_size, 1))\n",
        "    discriminator.trainable = False\n",
        "    gan.train_on_batch(noise, labels_mislabeled)\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "# Generate predictions using the generator\n",
        "# Generate predictions using the generator\n",
        "noise = np.random.normal(0, 1, size=(test_data.shape[0], noise_dim))\n",
        "predictions = generator.predict(noise)\n",
        "predictions = predictions.reshape(test_data.shape[0], -1) * (df[\"Market Values\"].max() - df[\"Market Values\"].min()) + df[\"Market Values\"].min()\n",
        "\n",
        "# Calculate mean squared error\n",
        "mse = mean_squared_error(test_data, predictions)\n",
        "print(\"MSE:\", mse)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCHPIfIqxLqt",
        "outputId": "ed240f58-52f2-4e65-a6b1-f0b93effbfcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 3ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 0s 5ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 3ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 3ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 5ms/step\n",
            "85/85 [==============================] - 0s 6ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 1s 5ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 1s 8ms/step\n",
            "85/85 [==============================] - 1s 8ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 6ms/step\n",
            "85/85 [==============================] - 1s 8ms/step\n",
            "85/85 [==============================] - 1s 8ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 5ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 0s 5ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 1s 7ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 3ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 1s 6ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 3ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "85/85 [==============================] - 0s 3ms/step\n",
            "85/85 [==============================] - 0s 4ms/step\n",
            "37/37 [==============================] - 0s 4ms/step\n",
            "MSE: 2.3474319155216214e+17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Avcr52t-yEVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msTzJfumihz6",
        "outputId": "9493b1b0-2a29-42a1-f57b-791b50658513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Series.count of 0       False\n",
              "1       False\n",
              "2       False\n",
              "3       False\n",
              "4       False\n",
              "        ...  \n",
              "3872    False\n",
              "3873    False\n",
              "3874    False\n",
              "3875    False\n",
              "3876    False\n",
              "Name: Market Value, Length: 3877, dtype: bool>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}